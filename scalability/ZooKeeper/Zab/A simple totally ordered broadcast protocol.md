# [A simple totally ordered broadcast protocol](https://www.datadoghq.com/pdf/zab.totally-ordered-broadcast-protocol.2008.pdf)

* ZooKeeper provides high read throughput by servicing the reads from the local replica of the ZooKeeper state at each server
* ZooKeeper write requests are sent to a single server, which transfroms the request into an idempotent transaction
* The leader can perform the transformation because it has a perfect view of the future state of the replicated database and can calculate the state of the new record

## Requirements

* If a message `m` is delivered by one server, it will eventually be delivered by all correct servers
* If a message `a` is delivered before message `b` by one server, then every server that delivers `a` and `b` delivers `a` before `b`
* If message `a` causally precedes message `b` and both messages are delivered, then `a` must be ordered before `b`
* If `m` is the last message delvered for a leader `L`, than any message proposed before `m` by `L` must also be delivered
* A process might be elected multiple times, however, each time counts as a different leader
* If two messages `a` and `b` are sent by the same server and `a` is proposed before `b`, we say that `a` causally precedes `b`
* Zab assumes a single leader server at a time that can commit proposals
  * If a leader changes, any previously proposed messages causally precede messages proposed by the new leader
* Messages are recorded on a quorum of disks before a messages is delivered
* ZooKeeper uses an in-memory database and stores transaction logs and periodic snapshots on disk
* Since the database is in-memory and we use gigabit interface cards, the bottleneck is the disk I/O on the writes
* To mitigate the disk I/O bottleneck, we write transactions in batches so that we can record multiple transactions in a single write to the disk
* This batching happens at the replicas not at the protocl level, so the implementation is much closer to group commits from databases than message packing
* When a server recovers, it is going to read it's snapshot and replay all delivered transactions after that snapshot
During recovery, the atomic broadcast does not need to guarantee at most once delivery since the use of idempotent transactions means that multiple delivery of a transaction is fine as long as on restart, order is preserved
  * If `a` is delivered before `b` and after a failure, `a` is redelivered, then `b` will also be redelivered  after `a`

## Protocol

* The ZooKeeper service uses a leader to process requests
* Any server other than the leader that needs to broadcast a message, first forwards the message to the leader
* Any quorom of followers are sufficient for a leader and the server to stay active
  * A Zab service made up of three servers (1 leader, 2 followers) will move to broadcast mode
  * If once of the followers die, there is no interruption in service, since the leader will still have a quorum
  * If the follower recovers and the other dies, there will still be no service interruption

### Broadcast

* 2-phase commit: a leader proposes a request, collects cotes, and finally commits
* We can commit once a quorum of servers ack the proposal rather than waiting for all servers to respond
* Broadcast uses FIFO (TCP) channels for all communications, which preserves ordering guarantees
* Before proposing a message, the leader assigns a monotonically increasing unique id (`zxid`)
* Because Zab preserves causal ordering (`a` is proposed before `b`), delivered messages will also be ordered by their `zxid`s
* Broadcasting consists of putting the proposal with the message attached into the outgoing queue from each follower to be sent through the FIFO channel
* When a follower receives a proposal, it writes it to disk, batching when it can, and sends an acknowledgement to the leader as soon as the proposal is on disk media
* When a leader receives ACKs from a quorum, the leader will broadcast a COMMIT and deliver the message locally
* Followers deliver the message when the receive the COMMIT from the leader

### Recovery

* Never forget delivered messages
* Let go of messages that are skipped
* A message that gets delvered on one machine must be delivered on all even if that machine fails
  * This situation can occur if the leader commits a message and then fails before the COMMIT reaches any other server
  * Because the leader committed the message, a client could have seen the result of the transaction in the message, so the transaction must be delivered to all other servers eventually so that the client sees a consistent view of the service
* A skipped message can occur when a proposal gets generated by a leader and the leader fails before anyone else sees the proposal
  * If later proposals are committed, then the message must be forgotten
* Leader election protocol guarantees that the new leader has the highest proposal number in a quorum of servers which also guarantees that the newly elected leader will also have all committed messages
* Before proposing any new messages, a newly elected leader makes sure that all messages that are in its transaction log have been propose, and committed, by a quorum of followers
* `zxid` is a 64-bit number
  * Lower 32-bits are used as a counter, where each proposal increments the counter
  * High 32-bits is the epoch
  * Each time a new leader takes over, it gets the epoch of the highest `zxid` in its log and increments the epoch, and uses this new epoch with a counter set to zero
* If a server that has been down, is restarted with a proposal that was never delivered from a previous epoch, it is not able to be a new leader, since every possible quorum of servers has a server with a proposal from a newer epoch and therefore, a higher `zxid`
  * When this server connects as a follower, the leader checks the last committed proposal for the epoch of the follower's latest proposal and tells the follower to truncate its transaction log to the last committed proposal for that epoch
